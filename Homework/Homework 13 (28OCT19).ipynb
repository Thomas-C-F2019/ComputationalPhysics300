{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Minimum of This Function Using Random Monte Carlo Method is:  2.9553687085087656e-08\n"
     ]
    }
   ],
   "source": [
    "#Thomas Conibear - First Optimization Homework \n",
    "\n",
    "#Monte Carlo Method to find minimum of a 2-D function\n",
    "#I will use the 2-D function right before the homework assignment to analyze\n",
    "\n",
    "import numpy\n",
    "#2-D function from notes\n",
    "def f(x,y):\n",
    "    return 1/2 * x ** 2 + 1/4 * y ** 2\n",
    "\n",
    "# First we will use the Monte Carlo method which relies on random numbers and calculations to find what you are looking for\n",
    "# So to find the minimum we will have to plug in thousands of random x and y values to get thousands of points\n",
    "# We then use these calculated points and select the minimum from these points to use as your minimum\n",
    "\n",
    "#Make empty list\n",
    "Points = []\n",
    "N = 10000000  #Number of points we want to calculate to take minimum of. Increase amount to increase accuracy\n",
    "# I tried to increase N but it was quite slow so I settled with this number\n",
    "\n",
    "for i in range(N):\n",
    "    x = numpy.random.random()\n",
    "    y = numpy.random.random()\n",
    "    points = f(x,y)\n",
    "    Points.append(points)\n",
    "    \n",
    "#We can use a built in minimum finder present in Python to find minimum of all the points calculated\n",
    "print(\"The Minimum of This Function Using Random Monte Carlo Method is: \", min(Points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we use the Gradient Descent Method on the same 2-D function to compare\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "def f(x,y):\n",
    "    return 1/2 * x ** 2 + 1/4 * y ** 2\n",
    "\n",
    "\n",
    "def gradient(f,x,y,dx=.001,dy=.001):\n",
    "    return (f(x+dx,0)-f(x,0))/dx + (f(0,y+dy)-f(0,y))/dy #Gradient for descent gradient method\n",
    "\n",
    "\n",
    "def minimize(f, x0, y0, N=100000):\n",
    "    \n",
    "    #Defining all the initial conditions\n",
    "    \n",
    "    y_current = y0\n",
    "    x_current = x0\n",
    "    converged = False\n",
    "    x_previous = None\n",
    "    y_previous = None\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        #Variable step calculations to descend down the slope of the curve to the minimum\n",
    "        \n",
    "        if x_previous == None and y_previous == None:\n",
    "            dx = .0001\n",
    "            dy = .0001\n",
    "        else:\n",
    "            dfy = dfx = (gradient(f,x_current,y_current)-gradient(f,x_previous,y_current))\n",
    "            dx = (x_current - x_previous)/dfx\n",
    "            dy = (y_current - y_previous)/dfy\n",
    "            \n",
    "        x_next = x_current - gradient(f,x_current,y_current)*dx\n",
    "        y_next = y_current - gradient(f,x_current,y_current)*dy\n",
    "\n",
    "        \n",
    "        if f(x_next,y_next) < f(x_current,y_current): #Checking if slope of current point is larger than next point to find minimum\n",
    "            x_previous = x_current\n",
    "            x_current = x_next\n",
    "            y_previous = y_current\n",
    "            y_current = y_next #Redifining all the new variables if we have not reached the minimum\n",
    "                \n",
    "        else:\n",
    "            converged = True\n",
    "            break\n",
    "    return converged, f(x_current,y_current)  #Returns the minimum point of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of this function using Gradient Descent method is:  7.496625379687501e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimum of this function using Gradient Descent method is: \", minimize(f, 0.001, 0.001, N=100000)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo method and the Gradient Descent method yielded close values. They are both incredibly small and are close to the analytic solution of the minimum being 0 as this function cannot be negative in any way due to the terms being squared. So the Gradient Descent and Monte Carlo methods are useful for getting minimum's close to the accepted value. In our case the Monte Carlo method was slightly more accurate but that could be due to it running 10 million times. I tried modifying N on the gradient descent but it did not affect the value whatsoever, this is likely due to an error in code on my part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
