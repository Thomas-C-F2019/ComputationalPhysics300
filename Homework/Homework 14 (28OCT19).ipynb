{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thomas Conibear - Second Optimization Homework\n",
    "\n",
    "The assignment says to use a program that is not a Scipy function to find the minimum of our given function. So I used the Gradient Descent code from the previous optimization assignment because it gave a somewhat accurate number and was a simple and easy to understand method of approximating a function's minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "def f(x,y):\n",
    "    return x**2/2 + y**2/3 - x*y/4\n",
    "\n",
    "\n",
    "def gradient(f,x,y,dx=.001,dy=.001):\n",
    "    return (f(x+dx,0)-f(x,0))/dx + (f(0,y+dy)-f(0,y))/dy #Gradient for descent gradient method\n",
    "\n",
    "\n",
    "def minimize(f, x0, y0, N=100000):\n",
    "    \n",
    "    #Defining all the initial conditions\n",
    "    \n",
    "    y_current = y0\n",
    "    x_current = x0\n",
    "    converged = False\n",
    "    x_previous = None\n",
    "    y_previous = None\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        #Variable step calculations to descend down the slope of the curve to the minimum\n",
    "        \n",
    "        if x_previous == None and y_previous == None:\n",
    "            dx = .0001\n",
    "            dy = .0001\n",
    "        else:\n",
    "            dfy = dfx = (gradient(f,x_current,y_current)-gradient(f,x_previous,y_current))\n",
    "            dx = (x_current - x_previous)/dfx\n",
    "            dy = (y_current - y_previous)/dfy\n",
    "            \n",
    "        x_next = x_current - gradient(f,x_current,y_current)*dx\n",
    "        y_next = y_current - gradient(f,x_current,y_current)*dy\n",
    "\n",
    "        \n",
    "        if f(x_next,y_next) < f(x_current,y_current): #Checking if slope of current point is larger than next point to find minimum\n",
    "            x_previous = x_current\n",
    "            x_current = x_next\n",
    "            y_previous = y_current\n",
    "            y_current = y_next #Redifining all the new variables if we have not reached the minimum\n",
    "                \n",
    "        else:\n",
    "            converged = True\n",
    "            break\n",
    "    return converged, f(x_current,y_current)  #Returns the minimum point of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum of this function using Gradient Descent method is:  5.83041703125e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimum of this function using Gradient Descent method is: \", minimize(f, 0.001, 0.001, N=100000)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 5\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 65\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 48\n",
      "         Function evaluations: 92\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 3\n",
      "         Function evaluations: 65\n"
     ]
    }
   ],
   "source": [
    "#Scipy method - Used the code and four methods from the notes.\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f2(x):\n",
    "    return x[0]**2/2 + x[1]**2/3 - x[0]*x[1]/4\n",
    "\n",
    "[x0, y0] = init(x_min, x_max, y_min, y_max)\n",
    "res = minimize(f2, [x0,y0], method='BFGS', tol=1e-4, options={'disp': True})\n",
    "\n",
    "res = minimize(f2, [x0,y0], method='powell', tol=1e-4, options={'disp': True})\n",
    "\n",
    "res = minimize(f2, [x0,y0], method='nelder-mead', tol=1e-4, options={'disp': True})\n",
    "\n",
    "res = minimize(f2, [x0,y0], method='powell', tol=1e-4, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the Gradient Descent method got a value that was accurate but not exact to the minimum of the function. Once again, the true minimum is zero due to the squares present in the function that always make the output positive despite the subtraction term. The four scipy methods give the exact solution however which demonstrates that scipy's code is more advanced and accurate than the Gradient Descent method I used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
